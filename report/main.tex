\documentclass[conference,compsoc]{IEEEtran}
\usepackage{hyperref}

\usepackage[justification=raggedright,format=hang]{caption}

\usepackage{url}
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{float}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\usepackage{epigraph}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{array}
\usepackage{authblk}
\usepackage[ruled,vlined]{algorithm2e}

\newcommand{\myparagraph}[1]{\smallskip\noindent{\bf #1.}}
\newcommand{\myfirstparagraph}[1]{\noindent{\bf #1.}}
\newcommand{\scedit}[1]{{\color{black} #1}} % blue
    
\begin{document}

\title{FuzzSplore: Visualizing Feedback-Driven Fuzzing Techniques}

\iffalse
\author{\IEEEauthorblockN{Andrea Fioraldi}
\IEEEauthorblockA{\textit{1692419}}
\and
\IEEEauthorblockN{Luigi Paolo Pileggi}
\IEEEauthorblockA{\textit{0000000}}
}

\author{\IEEEauthorblockN{Andrea Fioraldi\IEEEauthorrefmark{1}\IEEEauthorrefmark{2},
Luigi Paolo Pileggi\IEEEauthorrefmark{1}}
\IEEEauthorblockA{andreafioraldi@gmail.com,
gigimail@gigi.com\\
\IEEEauthorrefmark{1} Sapienza University of Rome,
\IEEEauthorrefmark{2} EURECOM}}
\fi

\author[1]{Andrea Fioraldi}
\author[1]{Luigi Paolo Pileggi}
\affil[1]{Sapienza University, Rome, Italy \authorcr {\tt \{fioraldi.1692419, pileggi.1691249\}@studenti.uniroma1.it}\vspace{1.5ex}}

\maketitle

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{img/full}
\caption{Complete view of the {\sc FuzzSplore} visual panel.}
\label{fig:full}
\end{figure*}

\begin{abstract}
Fuzz Testing techniques are the state of the art in software testing for security issues nowadays.
Their great effectiveness attracted the attention of researchers and hackers and involved them in developing a lot of new techniques to improve Fuzz Testing.
The evaluation and the cross-comparison of these technique is an almost open problem. In this paper we propose a human-driven approach to this problem based on information visualization.
We developed a prototype upon the {\sc AFL++} fuzzing framework, {\sc FuzzSplore}, that an analyst can use to get useful insights about different fuzzing configurations applied to a specific target in order to choose or tune the best technique during a fuzzing campaign.
\end{abstract}

\section{Introduction}

{\em Fuzz Testing} or {\em Fuzzing} is a family of techniques to automatically uncovers bugs in software.

Due to its effectiveness, much more efficient than other software testing techniques like {\it Symbolic Execution} \cite{redqueen} \cite{sebastian}, the research in this field is flourishing and several different techniques were developed to improve fuzz testing, both from academy and industry.

The evaluation and the comparison of these techniques, however, is a debatable matter \cite{fuzzeval}.

A common proxy is the comparison of the code coverage reached over time by each fuzzer, due to the fact that a fuzzer cannot find a bug if it does not explore at least the vulnerable code segment.
Another widely used metric is found bugs over time, but a bug can be found just thanks to randomism or by specific target-dependant actions taken by the fuzzer and this makes the evaluations very prone to overfitting.

The data collected using these metrics are often representable using a simple time-based graph that shows the evolution of the fuzzing algorithm.

This approach is useful for an immediate basic comparison between two or more techniques, an analyst have to just see which technique reaches more coverage in less time, but does not reveal the properties of a fuzzer regards specific types of programs states.

For instance, a technique can be better than another in exploring some types of program states and in the same time reaching less code coverage.
The technique will not cover the bugs in the unexplored code of course, but it may uncover bugs in the program points that it can better explore.
An example of such technique is the directed fuzzer towards sanitizers violations by \"Osterlund et al. \cite{parmesan}.

The problem of the evaluation of fuzzing techniques is important not only when the aim is to generally states which fuzzer is best, but also when an analyst wants to select the best fuzzers for a single target. It is common that fuzzers that are considered generally better than other on some targets performe worst than the others \cite{aflplusplus}.

We propose {\sc FuzzSplore}, a tool that allows an analyst to manually explore the evolution of different fuzzing techniques regards a single target program.

The main insight that an user can get using the tool are:

\begin{enumerate}
\item The ability of a fuzzer to generate clusters of inputs that are correlated in terms of covered program points;
\item The ability of a fuzzer in generating diversified inputs with its mutational algorithm;
\item The ability of a fuzzer to reach program points exploring intermediate inputs that are not an improvement in terms of coverage \cite{besensitive}.
\end{enumerate}

These insights can drive the user in the choice of the best technique to use for the selected program under test (PUT).

\section{Background}
\label{sec:back}

The simplest description of Feedback-driven Fuzzing is an algorithm that provides apparently random data to a computer program and then it watches for crashes or unexpected states and also saves the generated input for later processing if they covers interesting new states in terms of the chosen feedback \cite{fuzzing-book}.

Tipically, the property of the program used as feedback is the set of the edges in the program Control Flow Graph \cite{compilerbook} in what is the so-called {\it Coverage-guided Fuzzing} (CGF).

The inputs are mutations of previously saved inputs in the fuzzing loop in Mutational Fuzzing (Figure \ref{fig:cfg}) or generated from scratch from a model in Generational Fuzzing.

We base our implementation on {\sc AFL++} \cite{aflplusplus}, a widely used fuzzer in the recent times, that is a Mutational Coverage Guided Fuzzer.

\begin{figure}[H]
\includegraphics[scale=0.2]{img/cgf}
\centering
\caption{Basic representation of the Mutational Coverage Guided Fuzzing algorithm}
\label{fig:cfg}
\end{figure}

State of the art Coverage-Guided Fuzzers encode the approximate executed path in a representation that is easy and fast to process.
{\sc AFL++} uses a vector of 65536 entries by default, the {\it hitcounts} vector.

Each coordinate is associated with an edge and each value represents how many times the edge is executed modulo 256.

When a values greater than the previous one is registered in this vector, the fuzzer consider the input interesting and saves it.

Some extensions of CGF save also intermediate inputs that are a superset of the coverage reported in the hitcounts vector, like \cite{lafintel} \cite{ijon} \cite{besensitive}.

In general, when an input is saved, we can associate it to the testcases that generated it by mutation, the {\it parent} testcases. In this way, is easy to construct a graph of generated inputs that represents the progress of the hill climbing algorithm of the fuzzer, the {\it Generations} Graph.

% Looking for parents in this tree, we can evaluate if the intermediate inputs generated by the specific technique is effective.

\section{Methodology}

A {\em fuzzing campaign} is the process of running one or more fuzzers for a long period of time or even continously like in OSS-Fuzz \cite{serebryany2017oss}.

Security researchers tipically starts fuzzing using naive configurations and off-the-shelf fuzzers, then, meanwhile the campaign runs, observe the evolution and tune the fuzzers.

Our proposed approach aims to insert in the observation-tuning feedback loop a visual component to hekp the researcher in better understand insights about the fuzzers testing a particular target.

The data processed by {\em FuzzSplore} comes from the execution of the corpus of testcases that each fuzzer saved so far. The execution is instrumented and various properties are observed.

Then we visualize these collected properties and the user can relate them to better understand what is going. After that, the user can choose to drop some fuzzers if less effective and assign more resources (tipically CPUs) to the most effective fuzzers or tune each individual fuzzer.

The fuzzing campaing can then continue. When it saturates, the analyst can collect insights using our tool and restart the visual analytics feedback-loop.

Saturation of fuzzers, when no more additional state is explored or the number of states explodes, is a problem that was rarely addressed in academic literature but that affects each type of Feedback-driven Fuzzer \cite{saturation},  and a tool that can guide towards the selection of techniques that avoid saturation can help a lot the campaign.

\subsection{Data Retrieval}

% We developed a script to extract the data from the execution of the various instrumented programs used by each fuzzer.

We denote each fuzzer $F_i$ where $i$ is the index that identify it. 
With $PUT_i$ we denote the version of the PUT preprocessed and istrumented in order to be used by $F_i$.
$PUT_e$ is the version of the PUT that logs the edge coverage using the hitcounts vector. It has to be provided independently if it is used or not by some fuzzer $F_i$.
With $T_i(t)$ we denote the set of the saved testcases, the queue, by $F_i$ until time $t$ (seconds).

Given $t$ as the time chosen by the user to observe the progrss of the fuzzers, the Algorithm \ref{alg:data} computes the following sets:

\begin{itemize}
\item the set $C$ of all the functions $C_i \colon Time \longrightarrow NumEdges$ that relates, for the fuzzer $F_i$, a time unit to the number of discovered edges so far;
\item the set $I$ of all the functions $I_i \colon Testcase \longrightarrow \{F_j, ...\}$ that associates, for the fuzzer $F_i$, each testcase in $T_i(t)$ to the set of fuzzers that consider the testcase as interesting;
\item the set $X$ of the sets $X_i$, that maintains, for each fuzzer, the hitcounts vectors assciated with the execution of each testcase in $T_i(t)$;
\end{itemize}

\begin{algorithm}[h]
\DontPrintSemicolon
%\KwResult{The set $C$ of all the functions $C_i \colon Time \longrightarrow NumEdges$ that associates, for the fuzzer $F_i$, a time unit to the number of discovered edges so far. The set of all the functions $I_i \colon Testcase \longrightarrow \{F_j...\}$ that associates, for the fuzzer $F_i$, each testcase of $T_i(t)$ to the set of fuzzers that consider the testcase as interesting.}

  \For {$F_i$ \textbf{ in } $Fuzzers$}{
    $V_{acc} \gets (0_0 ... 0_{65536}) $\;
    
    \For {$T$ \textbf{ in } $T_i(t)$}{
      $V \gets Execute(PUT_e, T)$\;
      $X_i \gets X_i \cup \{V\}$\;
      $V, IsInteresting \gets MergeCoverage(V_{acc}, V)$\;
      \If {$IsInteresting$}{
        $C_i(Time(T)) \gets CountNotZeros(V_i)$\;
      }
    }
    
    \For {$F_j$ \textbf{ in } $Fuzzers \setminus F_i$}{
    
     $V_{acc} \gets (0_0 ... 0_{65536}) $\;
     
      \For {$T$ \textbf{ in } $T_i(t)$}{
        $V \gets Execute(PUT_j, T)$\;
        $V_{acc}, IsInteresting \gets MergeCoverage(V_{acc}, V)$\;
        \If {$IsInteresting$}{
          $I_i(T) \gets I_i(T) \cup \{F_j\}$\;
        }
      }
    
    }
    
  }

 \Return{$C, I, X$}\;
 \caption{Compute $C$, $I$, and $X$}
 \label{alg:data}
\end{algorithm}

The next item that has to be retrieved, in addition to $C$, $I$ and $X$, is the set $G$ of all the graphs $G_i$ that describes the evolution of each $T_i(t)$, the levels graph introduced in Sec. \ref{sec:back}.

We asume that each fuzzer encode the information about the parent testcases into the metadata of each testcase. In this way, it is trivial to construct the graph just reading all the metadata in $T_i(t)$.

\subsection{Visualization}

We visualize the computed data $C$, $I$, $X$, $G$ and some other properties that can be directly collected in four different views.

You can see these views with some example data in the screenshot of our implementation, in Figure \ref{fig:full}.

A timebar is used to select $t' \in [0, t]$ to ignore data ouside the selected time range and, for instance, visualize the data related to the queue $T_I(t')$ without the need to run again Algorithm \ref{alg:data}.

\subsubsection{Testcases Scatterplot}

Each $X_i$ is a matric of $|T_i(t)|$ rows in which each row is a vector of 65536 entries.

There raw numbers are raw to visualize. To handle this problem, we reduce the dimensionality of each vector $X_{i,j}$ from 65536 to 2, in order to be easily visualized in a scatterplot.

To do that, we chosen an algorithm that optimize the conservation of local distances after the dimensionality reduction, {\em t-SNE} \cite{maaten2008visualizing}. The nature of this algorithm is randomic, it needs to process $X$ entirely in order to get new vectors that are meaningfully comparable.

We experimentally observed on a test dataset that a perplexity of 30 is good enough.

The user can select groups of nodes interactively to highlight properties in the other visualiztions.

\subsubsection{Coverage Growth Plot}

$C$ can be visualized simply using a line plot with the X axis representing the domain, the time, and the Y axis the number of edges.

When a testcase is selected in the scatterplot or in the generations graph a vertical line appears at position $x$ where $x$ is the time in which the testcase was discovered.

\subsubsection{Interesting Testcases Plot}

Ths plot is used to visualize the evolution of the fuzzing algorithm in finding new testcases. The X axis represent time in seconds, the Y axis the number of new interesting testcases saved by the fuzzer in that second.
This information is directly contained in $T_i(t)$.

Here too, when a testcase is selected in the scatterplot or in the generations graph a vertical line appears at position $x$ where $x$ is the time in which the testcase was discovered.

\subsubsection{Generations Graph}

We visualize each Generations Graph $G_i$ combined with $I$. Given a fuzzer $F_j$ from the user, we highlight in graph $G_i$ each node associated with each testcase $T$ if $F_j \in I_i(T)$. In this way, the user can know if the evolution of $T_i(t)$ associated to the fuzzer $F_i$ is compatible with the selected $F_j$.

When a testcase is selected in the scatterplot, the border of the corresponding node in the graph is highlighted. The user can select additinal nodes or deselect nodes selected from the scatterplot. The scatterplot selection is sychinized in both ways witht the graph.

\subsection{Analyst Feedback}

The insights that an analyst can retrieve using the visualization are, but not limited to, the following:

\begin{itemize}
\item looking just at the scatterplot, the user can select a subset of fuzzers that explore different program points if the points related to eahc fuzzer in the graphs are clustered;
 
\end{itemize}

\subsection{Choosing and Tuning Fuzzers}

\section{Implementation}


\section{Conclusion}





\iffalse
\section{Proposal}

We propose a time-based visualization of the progress of different techniques based on {\sc AFL++}.

The first view will be a {\em navigable levels tree}. The user can select the technique in a scrollbar and view the associated tree.

Each node of the tree can be selected to view if it can be considered an interesting input for another technique. This will help in understanding the insight 3, were a tecnhique rech a program points thanks to an intermediate input that another technqiue cannot see as interesting.

There will be also a {\em scatterplot representing the inputs} in terms of covered state.
We collect the hitcounts vector of each input, reduce the dimensionality using PCA and then apply t-SNE on the vector with reduced dimensions to preserve outliners in these most important dimensions.

The color of each point will be related to the technique that generated the input.

In this scatterplot, the user can manually select a cluster of points that represents inputs with related paths.

Of course it can choose to consider only a subset of the techniques and hide that points from the view.

Doing that, will help to cope the insight number 1 looking at the density of the cluster for each different technique.

When doing this, the user can then choose to highlight in the scatterplot the inputs that are parents for the points in the cluster.

A technque that is able to generate inputs from other that are almost completely unrelated have a better mutational algorithm than a technqiue that generate inputs similar to the parents, this helps the analyst with insight number 2.

The tree an the scatterplot are synchronized. When a cluster is selected, the corresponding inputs are highlighted in the tree.

All these points can be hidden using a {\em time bar}. Selecting a specific lapse of time, only inputs founds by the fuzzer before that time are considered.

The other two views, related to the evolution in time, are the {\em coverage over time graph} and the {\em new interesting inputs per seconds graph}.

When a testcase is selected (both in the scatterplot or in the tree) a line appears in these graphs highlighing the time in which the testcase was discovered.

Coverage over time shows when an input that is important in terms of new coverage is found and so suggest the user where it should put the time bar for a targeted analysis.

Inputs per seconds is related with the levels tree. If a fuzzer generates too many intermediate testcases it can saturate the evolution of the alogrithm, so if we select an intermediate input in the levels tree and see an peak in this graph likely the technique suffers from path explosion.

In order to give an idea about dimensionality, we pick pcre2 version 183 (SVN) as example PUT. We compiled it with the AFL++ compiler wrapper and the addition of AddressSanitizer and UndefinedBehaviourSanitizer. After 12 hours of fuzzing, we got 14853 generated inputs in the fuzzer queue.

As said before, the hitcounts vector has 65536 dimensions, so in this simple case we have 973406208 elements.

\section{Usage}

An analyst that wants to compare different Fuzzing techniques on a target program will do the following:

\begin{enumerate}
\item Run the fuzzers (one for each technique) for the same amount time (more than 12h is recommended);
\item Use the tool to process the data from the fuzzing campaigns;
\item Use the visual analytics part of the tool to understand which subset of techniques are effective;
\item Restart the fuzzing campaign assigning computational power to only the chosen techniques;
\item The behavior of fuzzers in a long time can saturate, maybe retry from step 2 to reduce again the set of techniques.
\end{enumerate}

Saturation of fuzzer is a problem that was rarely addressed in academic literature but that affects each type of Feedback-driven Fuzzer \cite{saturation}.
Fuzzing campaigns can last months, or never ends, like for OSS-Fuzz \cite{serebryany2017oss}, the continuous fuzzing service for OSS software by Google, and a tool that can guide towards the selection of techniques that avoid saturation can help a lot the campaign.

Saturation is only one of the problems that can be addressed with {\sc FuzzSplore}, but it is the problem that makes multiple uses of the tool during the campaign necessary.
\fi

\bibliographystyle{IEEEtran}
\bibliography{bibliography}

\end{document}
